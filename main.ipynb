{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcavus/Age-Estimation/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "krzbanZM4Oht",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "CS 559: Deep Learning\n",
        "Homework: Age Estimation using TensorFlow\n",
        "Muhammed Cavusoglu (21400653) and Kemal Buyukkaya (21200496)\n",
        "'''\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "def cnn_model(features, labels, mode):\n",
        "    # Input Layer\n",
        "    # Reshape input to 4-D tensor: [batch_size, width, height, channels]\n",
        "    input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
        "    \n",
        "    # Conv1 Layer\n",
        "    # Compute 32 features using a 5x5 filter with ReLU activation.\n",
        "    # Padding is added to preserve width and height.\n",
        "    # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
        "    # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
        "    conv1 = tf.contrib.layers.conv2d(\n",
        "        inputs = input_layer,\n",
        "        num_outputs = 32,\n",
        "        kernel_size = [3, 3],\n",
        "        padding= 'SAME',\n",
        "        activation_fn = tf.nn.relu,\n",
        "        weights_initializer=tf.contrib.layers.xavier_initializer()\n",
        "    )\n",
        "    \n",
        "    # Pooling1 Layer\n",
        "    # Max pooling layer with a 2x2 filter and stride of 2\n",
        "    # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
        "    # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
        "    pool1 = tf.contrib.layers.max_pool2d(inputs = conv1, kernel_size = [2, 2], stride = 2)\n",
        "    \n",
        "    bn3 = tf.contrib.layers.batch_norm(inputs = pool1, activation_fn = None)\n",
        "\n",
        "    # Conv2 Layer\n",
        "    # Compute 64 features using a 5x5 filter.\n",
        "    # Padding is added to preserve width and height.\n",
        "    # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
        "    # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
        "    conv2 = tf.contrib.layers.conv2d(\n",
        "        inputs = bn3,\n",
        "        num_outputs = 64,\n",
        "        kernel_size = [3, 3],\n",
        "        padding= 'SAME',\n",
        "        activation_fn = tf.nn.relu,\n",
        "        weights_initializer=tf.contrib.layers.xavier_initializer()\n",
        "    )\n",
        "    \n",
        "\n",
        "    # Pooling 2 Layer\n",
        "    # Max pooling layer with a 2x2 filter and stride of 2\n",
        "    # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
        "    # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
        "    pool2 = tf.contrib.layers.max_pool2d(inputs = conv2, kernel_size = [2, 2], stride = 2)\n",
        "    \n",
        "    bn4 = tf.contrib.layers.batch_norm(inputs = pool2, activation_fn = None)\n",
        "    \n",
        "    # TODO: Conv3 Layer\n",
        "    \n",
        "    # TODO: Pooling 3 Layer\n",
        "    \n",
        "    # TODO: Conv4 Layer\n",
        "    \n",
        "    # TODO: Pooling 4 Layer\n",
        "    \n",
        "    # Flatten tensor into a batch of vectors\n",
        "    # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
        "    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
        "    pool2_flat = tf.reshape(bn4, [-1, 7 * 7 * 64])\n",
        "\n",
        "    # FC Layer with 1024 neurons\n",
        "    # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
        "    # Output Tensor Shape: [batch_size, 1024]\n",
        "    fc1 = tf.contrib.layers.fully_connected(inputs = pool2_flat, num_outputs = 1024, activation_fn = None, \n",
        "                                           weights_initializer=tf.contrib.layers.xavier_initializer())\n",
        "    \n",
        "    bn1 = tf.contrib.layers.batch_norm(inputs = fc1, activation_fn = tf.nn.relu)\n",
        "\n",
        "    fc2 = tf.contrib.layers.fully_connected(inputs = bn1, num_outputs = 1024, activation_fn = None, \n",
        "                                           weights_initializer=tf.contrib.layers.xavier_initializer())\n",
        "    \n",
        "    bn2 = tf.contrib.layers.batch_norm(inputs = fc2, activation_fn = tf.nn.relu)\n",
        "\n",
        "    # Dropout (0.6 probability for keeping the element)\n",
        "    dropout = tf.contrib.layers.dropout(inputs = bn2, keep_prob = 1, is_training = (mode == tf.estimator.ModeKeys.TRAIN))\n",
        "    \n",
        "    # Regression Layer\n",
        "    # Input Tensor Shape: [batch_size, 1024]\n",
        "    # Output Tensor Shape: [batch_size, 1]\n",
        "    regression = tf.contrib.layers.fully_connected(inputs = dropout, num_outputs = 1, activation_fn = None)\n",
        "    \n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "      return tf.estimator.EstimatorSpec(mode = mode, predictions = regression)\n",
        "    \n",
        "    # Loss function\n",
        "    loss = tf.losses.mean_squared_error(labels = labels, predictions = regression)\n",
        "    \n",
        "    # Configure the Training Op (for TRAIN mode)\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "      optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')\n",
        "      train_op = optimizer.minimize(\n",
        "          loss = loss,\n",
        "          global_step = tf.train.get_global_step())\n",
        "      return tf.estimator.EstimatorSpec(mode = mode, loss = loss, train_op = train_op)\n",
        "\n",
        "    # Add evaluation metrics (for EVAL mode)\n",
        "    eval_metric_ops = {\n",
        "        \"MAE\": tf.metrics.mean_absolute_error(labels = labels, predictions = regression)\n",
        "    }\n",
        "    \n",
        "    return tf.estimator.EstimatorSpec(mode = mode, loss = loss, eval_metric_ops = eval_metric_ops)\n",
        "\n",
        "\n",
        "def main(argv):\n",
        "    # Load the data\n",
        "    training_data, training_labels, validation_data, validation_labels, test_data, test_labels = _load_dataset()\n",
        "    \n",
        "    # Estimator\n",
        "    # cnn_model(training_data, training_labels, None)\n",
        "    # age_estimator = tf.contrib.learn.Estimator(model_fn = cnn_model, model_dir=\"/temp/age_est_convnet_model\")\n",
        "    age_estimator = tf.estimator.Estimator(model_fn = cnn_model)\n",
        "    \n",
        "    # TODO: Set up logs\n",
        "    # Set up logging for predictions\n",
        "    # Log the values in the \"Softmax\" tensor with label \"probabilities\"\n",
        "    #logging_hook = tf.train.LoggingTensorHook(tensors = {\"loss\" : loss, \"accuracy\" : accuracy}, every_n_iter = 50)\n",
        "\n",
        "    # Train the model\n",
        "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "        x = {\"x\": training_data},\n",
        "        y = training_labels,\n",
        "        batch_size = 256,\n",
        "        num_epochs = None,\n",
        "        shuffle=True)\n",
        "        \n",
        "    age_estimator.train(\n",
        "        input_fn = train_input_fn,\n",
        "        steps=1200)\n",
        "        #,hooks=[logging_hook])\n",
        "\n",
        "    # TODO: Evaluate the model and print results\n",
        "    #eval_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": test_data}, y = test_labels, num_epochs = 1, shuffle = False)\n",
        "    #eval_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": validation_data}, y = validation_labels, num_epochs = 1, shuffle = False)\n",
        "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": training_data}, y = training_labels, num_epochs = 1, shuffle = False)\n",
        "\n",
        "    eval_results = age_estimator.evaluate(input_fn = eval_input_fn)\n",
        "    print(eval_results)\n",
        "    \n",
        "def _load_dataset():\n",
        "    # Training set\n",
        "    training_data = []\n",
        "    training_labels = []\n",
        "    \n",
        "    tr_path = '/content/drive/My Drive/Colab Notebooks/UTKFace_downsampled/training_set'\n",
        "    for filename in os.listdir(tr_path):\n",
        "        training_labels.append(int(filename[:3]))\n",
        "        img_data = cv2.imread(os.path.join(tr_path, filename), cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "        img_data = cv2.resize(img_data, (28, 28)) # Resize\n",
        "        training_data.append(img_data)\n",
        "        \n",
        "    training_data = np.array(training_data, dtype='float32')\n",
        "    training_labels = np.array(training_labels, dtype='float32')\n",
        "    training_labels = training_labels.reshape(len(training_labels), 1)\n",
        "    \n",
        "    # Validation set\n",
        "    validation_data = []\n",
        "    validation_labels = []\n",
        "    \n",
        "    v_path = '/content/drive/My Drive/Colab Notebooks/UTKFace_downsampled/validation_set'\n",
        "    for filename in os.listdir(v_path):\n",
        "        validation_labels.append(int(filename[:3]))\n",
        "        img_data = cv2.imread(os.path.join(v_path, filename), cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "        img_data = cv2.resize(img_data, (28, 28)) # Resize\n",
        "        validation_data.append(img_data)\n",
        "        \n",
        "    validation_data = np.array(validation_data, dtype='float32')\n",
        "    validation_labels = np.array(validation_labels, dtype='float32')\n",
        "    validation_labels = validation_labels.reshape(len(validation_labels), 1)\n",
        "    \n",
        "    # Test set\n",
        "    test_data = []\n",
        "    test_labels = []\n",
        "    \n",
        "    t_path = '/content/drive/My Drive/Colab Notebooks/UTKFace_downsampled/test_set'\n",
        "    for filename in os.listdir(t_path):\n",
        "        test_labels.append(int(filename[:3]))\n",
        "        img_data = cv2.imread(os.path.join(t_path, filename), cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "        img_data = cv2.resize(img_data, (28, 28)) # Resize\n",
        "        test_data.append(img_data)\n",
        "        \n",
        "    test_data = np.array(test_data, dtype='float32')\n",
        "    test_labels = np.array(test_labels, dtype='float32')\n",
        "    test_labels = test_labels.reshape(len(test_labels), 1)\n",
        "    \n",
        "    return training_data, training_labels, validation_data, validation_labels, test_data, test_labels\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "  tf.app.run()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}