{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcavus/Age-Estimation/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "krzbanZM4Oht",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        },
        "outputId": "c7cf2b96-fc75-4101-b77e-853cd84d4770"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "CS 559: Deep Learning\n",
        "Homework: Age Estimation using TensorFlow\n",
        "Muhammed Cavusoglu (21400653) and Kemal Buyukkaya (21200496)\n",
        "'''\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For colab\n",
        "# '''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# '''\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "def cnn_model(features, labels, mode):\n",
        "    # Input Layer\n",
        "    # Reshape input to 4-D tensor: [batch_size, width, height, channels]\n",
        "    input_layer = tf.reshape(features[\"x\"], [-1, 88, 88, 1])\n",
        "    \n",
        "    # Conv1 Layer\n",
        "    # Compute 32 features using a 5x5 filter with ReLU activation.\n",
        "    # Padding is added to preserve width and height.\n",
        "    # Input Tensor Shape: [batch_size, 88, 88, 1]\n",
        "    # Output Tensor Shape: [batch_size, 88, 88, 32]\n",
        "    conv1 = tf.contrib.layers.conv2d(\n",
        "        inputs = input_layer,\n",
        "        num_outputs = 32,\n",
        "        kernel_size = [4, 4],\n",
        "        padding= 'SAME',\n",
        "        activation_fn = tf.nn.relu,\n",
        "        weights_initializer = tf.contrib.layers.xavier_initializer(),\n",
        "        weights_regularizer = tf.contrib.layers.l2_regularizer(0.8)\n",
        "    )\n",
        "    \n",
        "    # Pooling1 Layer\n",
        "    # Max pooling layer with a 2x2 filter and stride of 2\n",
        "    # Input Tensor Shape: [batch_size, 88, 88, 32]\n",
        "    # Output Tensor Shape: [batch_size, 44, 44, 32]\n",
        "    pool1 = tf.contrib.layers.max_pool2d(inputs = conv1, kernel_size = [2, 2], stride = 2)\n",
        "    \n",
        "    bn1 = tf.contrib.layers.batch_norm(inputs = pool1, activation_fn = None)\n",
        "\n",
        "    # Conv2 Layer\n",
        "    # Compute 64 features using a 5x5 filter.\n",
        "    # Padding is added to preserve width and height.\n",
        "    # Input Tensor Shape: [batch_size, 44, 44, 32]\n",
        "    # Output Tensor Shape: [batch_size, 44, 44, 64]\n",
        "    conv2 = tf.contrib.layers.conv2d(\n",
        "        inputs = bn1,\n",
        "        num_outputs = 64,\n",
        "        kernel_size = [4, 4],\n",
        "        padding= 'SAME',\n",
        "        activation_fn = tf.nn.relu,\n",
        "        weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "        weights_regularizer = tf.contrib.layers.l2_regularizer(0.8)\n",
        "    )\n",
        "    \n",
        "    # Pooling 2 Layer\n",
        "    # Max pooling layer with a 2x2 filter and stride of 2\n",
        "    # Input Tensor Shape: [batch_size, 44, 44, 64]\n",
        "    # Output Tensor Shape: [batch_size, 22, 22, 64]\n",
        "    pool2 = tf.contrib.layers.max_pool2d(inputs = conv2, kernel_size = [2, 2], stride = 2)\n",
        "    \n",
        "    bn2 = tf.contrib.layers.batch_norm(inputs = pool2, activation_fn = None)\n",
        "    \n",
        "    # Flatten tensor into a batch of vectors\n",
        "    # Input Tensor Shape: [batch_size, 22, 22, 64]\n",
        "    # Output Tensor Shape: [batch_size, 22 * 22 * 64]\n",
        "    pool2_flat = tf.reshape(bn2, [-1, 22 * 22 * 64])\n",
        "\n",
        "    # FC Layer with 1024 neurons\n",
        "    # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
        "    # Output Tensor Shape: [batch_size, 1024]\n",
        "    fc1 = tf.contrib.layers.fully_connected(inputs = pool2_flat, num_outputs = 1024, activation_fn = None, \n",
        "                                           weights_initializer=tf.contrib.layers.xavier_initializer())\n",
        "    \n",
        "    bn3 = tf.contrib.layers.batch_norm(inputs = fc1, activation_fn = tf.nn.relu)\n",
        "\n",
        "    fc2 = tf.contrib.layers.fully_connected(inputs = bn3, num_outputs = 1024, activation_fn = None, \n",
        "                                           weights_initializer=tf.contrib.layers.xavier_initializer())\n",
        "    \n",
        "    bn4 = tf.contrib.layers.batch_norm(inputs = fc2, activation_fn = tf.nn.relu)\n",
        "\n",
        "    # Dropout (0.6 probability for keeping the element)\n",
        "    dropout = tf.contrib.layers.dropout(inputs = bn4, keep_prob = 0.6, is_training = (mode == tf.estimator.ModeKeys.TRAIN))\n",
        "    \n",
        "    # Regression Layer\n",
        "    # Input Tensor Shape: [batch_size, 1024]\n",
        "    # Output Tensor Shape: [batch_size, 1]\n",
        "    regression = tf.contrib.layers.fully_connected(inputs = dropout, num_outputs = 1, activation_fn = None)\n",
        "    \n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "      return tf.estimator.EstimatorSpec(mode = mode, predictions = regression)\n",
        "    \n",
        "    # Loss function\n",
        "    # loss = tf.losses.mean_squared_error(labels = labels, predictions = regression)\n",
        "    loss = tf.losses.absolute_difference(labels = labels, predictions = regression)\n",
        "    \n",
        "    # Configure the Training Op (for TRAIN mode)\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "      optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')\n",
        "      train_op = optimizer.minimize(\n",
        "          loss = loss,\n",
        "          global_step = tf.train.get_global_step())\n",
        "      return tf.estimator.EstimatorSpec(mode = mode, loss = loss, train_op = train_op)\n",
        "\n",
        "    # Add evaluation metrics (for EVAL mode)\n",
        "    eval_metric_ops = {\n",
        "        \"MAE\": tf.metrics.mean_absolute_error(labels = labels, predictions = regression)\n",
        "    }\n",
        "    \n",
        "    return tf.estimator.EstimatorSpec(mode = mode, loss = loss, eval_metric_ops = eval_metric_ops)\n",
        "\n",
        "def main():\n",
        "    # Load the data\n",
        "    # Use below to re-generate npy files\n",
        "    #training_data, training_labels, validation_data, validation_labels, test_data, test_labels = _load_dataset()\n",
        "    \n",
        "    # For colab\n",
        "    #'''\n",
        "    training_data = np.load(\"/content/drive/My Drive/Colab Notebooks/training_data.npy\")\n",
        "    training_labels = np.load(\"/content/drive/My Drive/Colab Notebooks/training_labels.npy\")\n",
        "    validation_data = np.load(\"/content/drive/My Drive/Colab Notebooks/validation_data.npy\")\n",
        "    validation_labels = np.load(\"/content/drive/My Drive/Colab Notebooks/validation_labels.npy\")\n",
        "    test_data = np.load(\"/content/drive/My Drive/Colab Notebooks/test_data.npy\")\n",
        "    test_labels = np.load(\"/content/drive/My Drive/Colab Notebooks/test_labels.npy\")\n",
        "    #'''\n",
        "    \n",
        "    # For local \n",
        "    '''\n",
        "    training_data = np.load(\"training_data.npy\")\n",
        "    training_labels = np.load(\"training_labels.npy\")\n",
        "    validation_data = np.load(\"validation_data.npy\")\n",
        "    validation_labels = np.load(\"validation_labels.npy\")\n",
        "    test_data = np.load(\"test_data.npy\")\n",
        "    test_labels = np.load(\"test_labels.npy\")\n",
        "    '''\n",
        "    \n",
        "    # Estimator\n",
        "    age_estimator = tf.estimator.Estimator(model_fn = cnn_model)\n",
        "    \n",
        "    # Train the model\n",
        "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "        x = {\"x\": training_data},\n",
        "        y = training_labels,\n",
        "        batch_size = 256,\n",
        "        num_epochs = 1000,\n",
        "        shuffle = True\n",
        "    )\n",
        "        \n",
        "    age_estimator.train(\n",
        "        input_fn = train_input_fn,\n",
        "        steps = 1200\n",
        "    )\n",
        "\n",
        "    # Evaluate the model and print results\n",
        "    #eval_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": test_data}, y = test_labels, num_epochs = 1, shuffle = False)\n",
        "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": validation_data}, y = validation_labels, num_epochs = 1, shuffle = False)\n",
        "    #eval_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": training_data}, y = training_labels, num_epochs = 1, shuffle = False)\n",
        "\n",
        "    eval_results = age_estimator.evaluate(input_fn = eval_input_fn)\n",
        "    print(eval_results)\n",
        "    \n",
        "def _load_dataset():\n",
        "    # Training set\n",
        "    training_data = []\n",
        "    training_labels = []\n",
        "    \n",
        "    tr_path = 'UTKFace_downsampled/training_set'\n",
        "    for filename in os.listdir(tr_path):\n",
        "        training_labels.append(int(filename[:3]))\n",
        "        img_data = cv2.imread(os.path.join(tr_path, filename), cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "        img_data = cv2.resize(img_data, (88, 88)) # Resize\n",
        "        training_data.append(img_data)\n",
        "        \n",
        "    training_data = np.array(training_data, dtype='float32')\n",
        "    training_labels = np.array(training_labels, dtype='float32')\n",
        "    training_labels = training_labels.reshape(len(training_labels), 1)\n",
        "    np.save('training_data', training_data)\n",
        "    np.save('training_labels', training_labels)\n",
        "    \n",
        "    # Validation set\n",
        "    validation_data = []\n",
        "    validation_labels = []\n",
        "    \n",
        "    v_path = 'UTKFace_downsampled/validation_set'\n",
        "    for filename in os.listdir(v_path):\n",
        "        validation_labels.append(int(filename[:3]))\n",
        "        img_data = cv2.imread(os.path.join(v_path, filename), cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "        img_data = cv2.resize(img_data, (88, 88)) # Resize\n",
        "        validation_data.append(img_data)\n",
        "        \n",
        "    validation_data = np.array(validation_data, dtype='float32')\n",
        "    validation_labels = np.array(validation_labels, dtype='float32')\n",
        "    validation_labels = validation_labels.reshape(len(validation_labels), 1)\n",
        "    np.save('validation_data', validation_data)\n",
        "    np.save('validation_labels', validation_labels)\n",
        "    \n",
        "    # Test set\n",
        "    test_data = []\n",
        "    test_labels = []\n",
        "    \n",
        "    t_path = 'UTKFace_downsampled/test_set'\n",
        "    for filename in os.listdir(t_path):\n",
        "        test_labels.append(int(filename[:3]))\n",
        "        img_data = cv2.imread(os.path.join(t_path, filename), cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "        img_data = cv2.resize(img_data, (88, 88)) # Resize\n",
        "        test_data.append(img_data)\n",
        "        \n",
        "    test_data = np.array(test_data, dtype='float32')\n",
        "    test_labels = np.array(test_labels, dtype='float32')\n",
        "    test_labels = test_labels.reshape(len(test_labels), 1)\n",
        "    np.save('test_data', test_data)\n",
        "    np.save('test_labels', test_labels)\n",
        "    \n",
        "    return training_data, training_labels, validation_data, validation_labels, test_data, test_labels\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "  main()\n",
        "  "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpqortssq2\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpqortssq2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fea4bb7fcc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpqortssq2/model.ckpt.\n",
            "INFO:tensorflow:loss = 32.430603, step = 0\n",
            "INFO:tensorflow:global_step/sec: 6.54046\n",
            "INFO:tensorflow:loss = 5.2535396, step = 100 (15.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 6.58258\n",
            "INFO:tensorflow:loss = 4.206299, step = 200 (15.197 sec)\n",
            "INFO:tensorflow:global_step/sec: 6.56948\n",
            "INFO:tensorflow:loss = 2.8482714, step = 300 (15.217 sec)\n",
            "INFO:tensorflow:global_step/sec: 6.56991\n",
            "INFO:tensorflow:loss = 2.652276, step = 400 (15.221 sec)\n",
            "INFO:tensorflow:global_step/sec: 6.56679\n",
            "INFO:tensorflow:loss = 2.8869088, step = 500 (15.226 sec)\n",
            "INFO:tensorflow:global_step/sec: 6.63286\n",
            "INFO:tensorflow:loss = 2.88107, step = 600 (15.077 sec)\n",
            "INFO:tensorflow:global_step/sec: 6.57805\n",
            "INFO:tensorflow:loss = 2.3643107, step = 700 (15.204 sec)\n",
            "INFO:tensorflow:global_step/sec: 6.57817\n",
            "INFO:tensorflow:loss = 2.5501952, step = 800 (15.200 sec)\n",
            "INFO:tensorflow:global_step/sec: 6.57305\n",
            "INFO:tensorflow:loss = 2.0126553, step = 900 (15.216 sec)\n",
            "INFO:tensorflow:global_step/sec: 6.56982\n",
            "INFO:tensorflow:loss = 2.0015905, step = 1000 (15.223 sec)\n",
            "INFO:tensorflow:global_step/sec: 6.61553\n",
            "INFO:tensorflow:loss = 2.1166952, step = 1100 (15.115 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1200 into /tmp/tmpqortssq2/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 1.7293177.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-04-01T18:25:59Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmpqortssq2/model.ckpt-1200\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2019-04-01-18:26:00\n",
            "INFO:tensorflow:Saving dict for global step 1200: MAE = 6.446338, global_step = 1200, loss = 6.550968\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1200: /tmp/tmpqortssq2/model.ckpt-1200\n",
            "{'MAE': 6.446338, 'loss': 6.550968, 'global_step': 1200}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}